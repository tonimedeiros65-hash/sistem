{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0d0b1f",
   "metadata": {},
   "source": [
    "# Antonio Medeiros\n",
    "\n",
    "# Numero 109227\n",
    "\n",
    "#### Link para o Github\n",
    "\n",
    "https://github.com/tonimedeiros65-hash/sistem.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dde996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score,classification_report\n",
    "import skfuzzy as fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb987471",
   "metadata": {},
   "source": [
    "Para a escolha dos datasets é só necessário retirar o cardinal do dataset que se pertende estudar e adicionar um cardinal ao outro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "378a4483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHOOSE DATASET\n",
    "\n",
    "# Binary classification dataset\n",
    "#data = datasets.fetch_openml(name=\"diabetes\",version=1, as_frame=True)\n",
    "\n",
    "# Regression dataset \n",
    "data = datasets.load_diabetes(as_frame=True)\n",
    "\n",
    "X = data.data.values\n",
    "y = data.target.values\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f56c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(X)==768): # o proposito desta celula é para trasformar os dados compostos por \"tested_negative\" e \"tested_positive\" \n",
    "    # em 1 e 0, respetivamente.\n",
    "    y1=[]\n",
    "    for i in range(768):\n",
    "        if(y[i]==\"tested_negative\"):\n",
    "            y1=y1+[1]\n",
    "        else:\n",
    "            y1=y1+[0]\n",
    "    y1=np.array(y1)\n",
    "    y=y1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5716dbc",
   "metadata": {},
   "source": [
    "Isto divide o conjunto de dados de diabetes em 80% para treino e 20% para teste, representado pelo \"test_size\". O \"random_state\" representa uma semente aleatória fixa para garantir a reprodutibilidade. Desta forma, mesmo em dispositivos diferentes, os resultados serão os mesmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09853307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test spliting\n",
    "test_size=0.2\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a677f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler=StandardScaler()\n",
    "Xtr= scaler.fit_transform(Xtr)\n",
    "Xte= scaler.transform(Xte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e63a7",
   "metadata": {},
   "source": [
    "O número de clusters e o m foram escolhidos por tentativa e erro, sendo que estes valores foram os que produziram os melhores resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d296754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters \n",
    "if(len(X)==768): # \"n_clusters\" e o \"m\" para o problema binario\n",
    "    n_clusters = 2\n",
    "    m=1.7\n",
    "else: # \"n_clusters\" e o \"m\" para o problema da regressao\n",
    "    n_clusters = 4\n",
    "    m=1.1\n",
    "\n",
    "\n",
    "# Concatenate target for clustering\n",
    "Xexp=np.concatenate([Xtr, ytr.reshape(-1, 1)], axis=1)\n",
    "\n",
    "# Transpose data for skfuzzy (expects features x samples)\n",
    "Xexp_T = Xexp.T \n",
    "\n",
    "# Fuzzy C-means clustering\n",
    "centers, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(Xexp_T, n_clusters, m=m, error=0.005, maxiter=1000, init=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329dc2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0773b56",
   "metadata": {},
   "source": [
    "Nesta celula é calculada o sigma de cada cluster apartir de um processo de iteração representado pelo ciclo \"j\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dfe17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sigma (spread) for each cluster\n",
    "sigmas = []\n",
    "for j in range(n_clusters):\n",
    "    # membership weights for cluster j, raised to m\n",
    "    u_j = u[j, :] ** m\n",
    "    # weighted variance for each feature\n",
    "    var_j = np.average((Xexp - centers[j])**2, axis=0, weights=u_j)\n",
    "    sigma_j = np.sqrt(var_j)\n",
    "    sigmas.append(sigma_j)\n",
    "sigmas=np.array(sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8683b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard clustering from fuzzy membership\n",
    "cluster_labels = np.argmax(u, axis=0)\n",
    "print(\"Fuzzy partition coefficient (FPC):\", fpc)\n",
    "\n",
    "# Plot first two features with fuzzy membership\n",
    "plt.figure(figsize=(8,6))\n",
    "for j in range(n_clusters):\n",
    "    plt.scatter(\n",
    "        Xexp[cluster_labels == j, 0],             # Feature 1\n",
    "        Xexp[cluster_labels == j, 1],             # Feature 2\n",
    "        alpha=u[j, :],          # transparency ~ membership\n",
    "        label=f'Cluster {j}'\n",
    "    )\n",
    "\n",
    "plt.title(\"Fuzzy C-Means Clustering (with membership degree)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot first two features with cluster assignments\n",
    "plt.figure(figsize=(8,6))\n",
    "for j in range(n_clusters):\n",
    "    plt.scatter(\n",
    "        Xexp[cluster_labels == j, 0],\n",
    "        Xexp[cluster_labels == j, 1],\n",
    "        label=f'Cluster {j}'\n",
    "    )\n",
    "\n",
    "plt.title(\"Fuzzy C-Means Clustering (CRISPEN)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60524380",
   "metadata": {},
   "source": [
    "Com esta celula é calculado o centro das distribuições dos clusters para mehlor analizar as membership function de cada cluster projetado em cada \"feature\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb19243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian formula\n",
    "def gaussian(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu)/sigma)**2)\n",
    "\n",
    "lin=np.linspace(-2, 4, 500)\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "y_aux=[]\n",
    "for j in range(n_clusters):\n",
    "# Compute curves\n",
    "    y_aux.append(gaussian(lin, centers[j,0], sigmas[j,0]))\n",
    "\n",
    "# Plot\n",
    "    plt.plot(lin, y_aux[j], label=f\"Gaussian μ={np.round(centers[j,0],2)}, σ={np.round(sigmas[j,0],2)}\")\n",
    "# np.round(centers[j,a],2) --> a dita a dimensao que estamos a visualizar\n",
    "plt.title(\"Projection of the membership functions on Feature 1\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Degree of Membership\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e324f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Gaussian Membership Function\n",
    "# ---------------------------\n",
    "class GaussianMF(nn.Module):\n",
    "    def __init__(self, centers, sigmas, agg_prob):\n",
    "        super().__init__()\n",
    "        self.centers = nn.Parameter(torch.tensor(centers, dtype=torch.float32))\n",
    "        self.sigmas = nn.Parameter(torch.tensor(sigmas, dtype=torch.float32))\n",
    "        self.agg_prob=agg_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expand for broadcasting\n",
    "        # x: (batch, 1, n_dims), centers: (1, n_rules, n_dims), sigmas: (1, n_rules, n_dims)\n",
    "        diff = abs((x.unsqueeze(1) - self.centers.unsqueeze(0))/self.sigmas.unsqueeze(0)) #(batch, n_rules, n_dims)\n",
    "\n",
    "        # Aggregation\n",
    "        if self.agg_prob:\n",
    "            dist = torch.norm(diff, dim=-1)  # (batch, n_rules) # probablistic intersection\n",
    "        else:\n",
    "            dist = torch.max(diff, dim=-1).values  # (batch, n_rules) # min intersection (min instersection of normal funtion is the same as the max on dist)\n",
    "        \n",
    "        return torch.exp(-0.5 * dist ** 2)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# TSK Model\n",
    "# ---------------------------\n",
    "class TSK(nn.Module): \n",
    "    def __init__(self, n_inputs, n_rules, centers, sigmas,agg_prob=False):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_rules = n_rules\n",
    "\n",
    "        # Antecedents (Gaussian MFs)\n",
    "        \n",
    "        self.mfs=GaussianMF(centers, sigmas,agg_prob) \n",
    "\n",
    "        # Consequents (linear functions of inputs)\n",
    "        # Each rule has coeffs for each input + bias\n",
    "        self.consequents = nn.Parameter(\n",
    "            torch.randn(n_inputs + 1,n_rules)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, n_inputs)\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Compute membership values for each input feature\n",
    "        # firing_strengths: (batch, n_rules)\n",
    "        firing_strengths = self.mfs(x)\n",
    "        \n",
    "        # Normalize memberships\n",
    "        # norm_fs: (batch, n_rules)\n",
    "        norm_fs = firing_strengths / (firing_strengths.sum(dim=1, keepdim=True) + 1e-9)\n",
    "\n",
    "        # Consequent output (linear model per rule)\n",
    "        x_aug = torch.cat([x, torch.ones(batch_size, 1)], dim=1)  # add bias\n",
    "\n",
    "        rule_outputs = torch.einsum(\"br,rk->bk\", x_aug, self.consequents)  # (batch, rules)\n",
    "        # Weighted sum\n",
    "        output = torch.sum(norm_fs * rule_outputs, dim=1, keepdim=True)\n",
    "\n",
    "        return output, norm_fs, rule_outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48073544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Celula de Treino\n",
    "# ---------------------------\n",
    "\n",
    "# ---------------------------\n",
    "# Least Squares Solver for Consequents (TSK)\n",
    "# ---------------------------\n",
    "def train_ls(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        _, norm_fs, _ = model(X)\n",
    "\n",
    "        # Design matrix for LS: combine normalized firing strengths with input\n",
    "        X_aug = torch.cat([X, torch.ones(X.shape[0], 1)], dim=1)\n",
    "        \n",
    "        Phi = torch.einsum(\"br,bi->bri\", X_aug, norm_fs).reshape(X.shape[0], -1)\n",
    "        \n",
    "        # Solve LS: consequents = (Phi^T Phi)^-1 Phi^T y\n",
    "        \n",
    "        theta= torch.linalg.lstsq(Phi, y).solution\n",
    "    \n",
    "        \n",
    "        model.consequents.data = theta.reshape(model.consequents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66221c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = TSK(n_inputs=Xtr.shape[1], n_rules=n_clusters, centers=centers[:,:-1], sigmas=sigmas[:,:-1])\n",
    "\n",
    "Xtr = torch.tensor(Xtr, dtype=torch.float32)\n",
    "ytr = torch.tensor(ytr, dtype=torch.float32)\n",
    "Xte = torch.tensor(Xte, dtype=torch.float32)\n",
    "yte = torch.tensor(yte, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d11eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with LS:\n",
    "train_ls(model, Xtr, ytr.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10f12b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:2476.3916015625\n"
     ]
    }
   ],
   "source": [
    "y_pred, _, _=model(Xte)\n",
    "if(len(X)==768):\n",
    "    print(f'ACC:{accuracy_score(yte.detach().numpy(),y_pred.detach().numpy()>0.5)}') #classification\n",
    "else:\n",
    "    print(f'ACC:{mean_squared_error(yte.detach().numpy(),y_pred.detach().numpy())}') #regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ef5d4",
   "metadata": {},
   "source": [
    "Os resultados obtidos foram:\n",
    "\n",
    "--> 0.7857 para o \"Pima Indians Diabetes Dataset\"\n",
    "\n",
    "--> 2476.4 para o \"Diabetes Dataset\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
